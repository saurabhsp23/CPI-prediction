{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from sklearn.metrics import r2_score, mean_absolute_error,mean_absolute_percentage_error\n",
    "from sklearn.linear_model import Lasso\n",
    "import xgboost as xgb\n",
    "import inspect\n",
    "from scipy.fft import fft, ifft\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import f\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvalidOperation(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    LOOKBACK_PERIOD = 70\n",
    "    DATA_BASE_PATH = '../data'\n",
    "    # Includes all testing models here for later use\n",
    "    MODELS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepwiseRegression:\n",
    "    def __init__(self, criterion='aic', verbose=False):\n",
    "        self.criterion = criterion\n",
    "        self.verbose = verbose\n",
    "        self.features = []\n",
    "        self.model = LinearRegression()\n",
    "\n",
    "    def fit(self, X, y, stop_criterion=None):\n",
    "        n = X.shape[0]\n",
    "        p = X.shape[1]\n",
    "        self.features = []  # To store the selected features\n",
    "\n",
    "        if stop_criterion is None:\n",
    "            stop_criterion = p\n",
    "\n",
    "        if self.criterion == 'aic':\n",
    "            best_criterion = np.inf\n",
    "        else:\n",
    "            best_criterion = -np.inf\n",
    "\n",
    "        for _ in range(stop_criterion):\n",
    "            remaining_features = [f for f in range(p) if f not in self.features]\n",
    "            if len(remaining_features) == 0:\n",
    "                break\n",
    "\n",
    "            best_feature = None\n",
    "            best_model = None\n",
    "\n",
    "            for feature in remaining_features:\n",
    "                selected_features = self.features + [feature]\n",
    "                X_selected = X[:, selected_features]\n",
    "                X_selected = sm.add_constant(X_selected)\n",
    "                model = sm.OLS(y, X_selected).fit()\n",
    "\n",
    "                if self.criterion == 'aic':\n",
    "                    current_criterion = model.aic\n",
    "                    if current_criterion < best_criterion:\n",
    "                        best_criterion = current_criterion\n",
    "                        best_feature = feature\n",
    "                        best_model = model\n",
    "                else:  # Default to BIC\n",
    "                    current_criterion = model.bic\n",
    "                    if current_criterion > best_criterion:\n",
    "                        best_criterion = current_criterion\n",
    "                        best_feature = feature\n",
    "                        best_model = model\n",
    "\n",
    "            if best_feature is not None:\n",
    "                if self.verbose:\n",
    "                    print(f\"Adding feature {best_feature} to the model\")\n",
    "                self.features.append(best_feature)\n",
    "                self.model = best_model\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_selected = X[:, self.features]\n",
    "        X_selected_with_const = sm.add_constant(X_selected, has_constant='add')\n",
    "        return self.model.predict(X_selected_with_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Combo():\n",
    "    def __init__(self, df=None, agg_func=None, filt_func=None, smooth_func=None, tsagg_func=None, tester=None, lookback_period=None):\n",
    "        \n",
    "        self.df = df\n",
    "        self.agg_func = agg_func\n",
    "        self.filt_func = filt_func\n",
    "        self.smooth_func = smooth_func\n",
    "        self.tsagg_func = tsagg_func\n",
    "        self.test = tester\n",
    "        self.lookback_period = lookback_period\n",
    "        \n",
    "        \n",
    "        \n",
    "    """\n",
    "    Processes the dataframe passed in with the aggregate function, filter function, then smooth function\n",
    "    in that order. Can eventually update this to allow for multiple aggregate, filter, and smoothing operations\n",
    "    or to have the order of operations changed.\n",
    "    """\n",
    "    \n",
    "    def process_df(self):\n",
    "        try:\n",
    "            if self.agg_func:\n",
    "                self.df = self.agg_func(self.df)\n",
    "        except:\n",
    "            print('Aggregate function input/output error')\n",
    "        \n",
    "        try:\n",
    "            if self.filt_func:\n",
    "                self.df = self.filt_func(self.df)\n",
    "        except:\n",
    "            print('Filter function input/output error')\n",
    "        \n",
    "        try: \n",
    "            if self.smooth_func:\n",
    "                self.df = self.smooth_func(self.df)\n",
    "        except:\n",
    "            print('Smooth function input/output error')\n",
    "\n",
    "            \n",
    "            \n",
    "    def performance_metrics(self, y_pred, y_act):\n",
    "        """\n",
    "        Print performance metrics such as R-squared, MAE, and MAPE.\n",
    "\n",
    "        Args:\n",
    "            y_pred (list): Predicted values.\n",
    "            y_act (list): Actual values.\n",
    "        """\n",
    "        rsquared = r2_score(y_pred,y_act)\n",
    "        mae = mean_absolute_error(y_pred,y_act)\n",
    "        mape = mean_absolute_percentage_error(y_pred,y_act)\n",
    "\n",
    "        # calculating f stat\n",
    "        var_oos = np.var(y_act)\n",
    "        var_pred = np.var(y_pred)\n",
    "\n",
    "        # Perform F-test\n",
    "        F_statistic = var_pred / var_oos\n",
    "\n",
    "        # Calculate degrees of freedom\n",
    "        dof_pred = len(y_pred) - 1\n",
    "        dof_oos = len(y_act) - 1\n",
    "\n",
    "        # Calculate p-value\n",
    "        p_value = f.sf(F_statistic, dof_pred, dof_oos)\n",
    "\n",
    "        print(f'R-squared: {rsquared}, MAE: {mae}, MAPE: {mape}, p_value: {p_value}')\n",
    "        \n",
    "        return (rsquared, mae, mape, p_value)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def run_model(self, model = LinearRegression(), label = 'LinearRegression'):\n",
    "        """\n",
    "        Walk forward train/validate split using a machine learning model.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Combined DataFrame with monthly aggregated data and indicator data.\n",
    "            lookback_period (int): The number of periods to lo/ok back for training.\n",
    "            model: The machine learning model to use for predictions.\n",
    "        """\n",
    "        \n",
    "        lookback_period = self.lookback_period\n",
    "        df=self.df\n",
    "        \n",
    "        df = df.ffill()#.dropna()\n",
    "        df = df.dropna(axis=1, how='all')  # drop cols with all Nans\n",
    "        df=df.replace(np.nan, 0) # now dropping any intial rows with nans\n",
    "        total_periods = len(df)\n",
    "        y_pred = []  # predicted values\n",
    "        y_act = []  # actual values\n",
    "        indicator = df.columns[-1]\n",
    "        reg_betas = pd.DataFrame(index=df.columns[:-1])\n",
    "\n",
    "        for i in range(total_periods - lookback_period + 1):\n",
    "            df_lookback = df.iloc[i:i + lookback_period + 1]\n",
    "\n",
    "            # TODO IMPORTANT NOTE: think more carefully how we fill nans in the below instead of a\n",
    "            # back-fill, forward-fill\n",
    "            # df_lookback = df_lookback.ffill() # backfill nan's currently ... how to make better?\n",
    "            # and forward fill the final row\n",
    "            if len(df_lookback.columns) <= 1:\n",
    "                continue\n",
    "\n",
    "            df_train = df_lookback.head(-1)\n",
    "            df_test = df_lookback.tail(1)\n",
    "            y_act.append(float(df_test.iloc[:, -1].values))\n",
    "            # train linear regression model and get predicted value\n",
    "            xtrain = df_train.iloc[:, 0:-1]\n",
    "            ytrain = df_train.iloc[:, -1]\n",
    "            xtest = df_test.iloc[:, :-1]  # predictors\n",
    "\n",
    "            if label == 'XGBoosting':\n",
    "                # Use XGBoost specific code\n",
    "                xg_train = xgb.DMatrix(xtrain.values, label=ytrain.values)\n",
    "                xg_test = xgb.DMatrix(xtest.values)\n",
    "                params = {\n",
    "                    'tree_method': 'auto',  # Ensure tree_method is set to \"auto\"\n",
    "                    'objective': 'reg:squarederror',  # Use regression objective\n",
    "                }\n",
    "                model = xgb.train(params, xg_train)\n",
    "                # Update y_pred using XGBoost predictions\n",
    "                y_pred.append(float(model.predict(xg_test)[0]))\n",
    "            else:\n",
    "                model.fit(xtrain.values, ytrain.values)\n",
    "                # sm_lasso = sm.OLS(ytrain.values, xtrain.values)\n",
    "                ypred = model.predict(xtest.values)\n",
    "                y_pred.append(float(ypred))\n",
    "\n",
    "                model_params = pd.DataFrame(model.coef_, index=list(xtrain.columns))\n",
    "                reg_betas.loc[model_params.index, xtest.index] = model_params.values\n",
    "        if label == 'XGBoosting':\n",
    "                xgb.plot_importance(model)\n",
    "                plt.show()\n",
    "                importance = model.get_score(importance_type='weight')\n",
    "                \n",
    "        return self.performance_metrics(y_pred, y_act), reg_betas\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agg():\n",
    "    """\n",
    "    Class containing all the different aggregate functions. When adding new functions, just add a new static function\n",
    "    to the class and give it a distinct name. All the input outputs of the functions must be of the same format\n",
    "    unless in special cases, but then the user must be careful with handling the rest of the pipeline.\n",
    "    """\n",
    "\n",
    "    TREAT_AS_MULTI = {96: [1109, 1110, 1111], 316: [1820, 1821, 1822, 1823], 597: [1808, 1809, 1810, 1811, 1812, 1813], 664: [1827, 1829, 1830, 1831, 1832, 1833]}\n",
    "    GRADIENT = {97: [], 98: [], 101: [], 103: [], 104: [], 105: [], 106: [], 107: [], 108: [], 109: [], 110: [], 153: [], 154: [], 155: [], 156: [], 158: [], 159: [], 170: [], 173: [], 182: [], 183: [], 316: [1820, 1821, 1822, 1823, 1824], 335: [], 337: [], 338: [], 339: [], 340: [], 341: [], 342: [], 583: [], 584: [], 585: [], 586: [1090], 587: [1093], 595: [], 663: [], 664: [1827, 1829, 1830, 1831, 1832, 1833, 1834], 2400: [], 2401: [], 2402: [], 2403: [], 2406: [], 2428: [], 2429: [3370], 2464: [3671], 2465: [3677], 2466: [3683, 3684], 2467: [3690], 2468: [3695], 2474: [3742]}\n",
    "    UNCERTAINTY = {585:1087, 586:1090, 587:1093, 2429:3370, 2464:3671, 2465:3677, 2466:3683, 2467:3690, 2468:3695, 2474:3742}\n",
    "    SUBCAT = {'Economic Expectations': [2464, 2465, 2466, 2467, 2468],\n",
    "                'Economic Trends': [338, 339, 340, 341, 342, 583, 584, 585, 586, 587],\n",
    "                'Employment': [316,  664, 2406, 2407],\n",
    "                'Employment Effects': [597],\n",
    "                'General Shopping Trends': [182, 183],\n",
    "                'Grocery': [595],\n",
    "                'Home Improvement': [100, 663],\n",
    "                'Home Ownership': [ 96, 101],\n",
    "                'Home Value Prediction': [97, 98],\n",
    "                'Inflation': [2426, 2427, 2428, 2429, 2474],\n",
    "                'Online Retail': [170, 173],\n",
    "                'Personal Finances': [335,  337, 2400, 2401, 2402, 2403],\n",
    "                'Physical Retail': [153, 154, 155, 156, 158, 159, 184],\n",
    "                'Spending Expectations': [103, 104, 105, 106, 107, 108, 109, 110]}\n",
    "    \n",
    "    \n",
    "    def __init__(self, combinations):\n",
    "        self.combinations = combinations\n",
    "    \n",
    "    """\n",
    "    run_agg_process: Takes in a list of lists that provides the names of the functions to aggregate. Inner lists\n",
    "    are 'steps' and they do the listed aggregations in parallel and then append the resulting columns.\n",
    "    Each subsequent step will then do new aggregations based on the dataframe produced by the previous step rather\n",
    "    than on the original dataframe until the procedure is over.\n",
    "    Example: [['agg1', 'agg2'], ['agg3', 'agg4']]\n",
    "    In that example, agg1 and agg2 will be applied on the dataframe and those outputs will be concatenated\n",
    "    horizontally. Then, agg3 and agg4 will be run on that resulting dataframe and those outputs will be concatenated\n",
    "    horizontally again as the final output.\n",
    "    \n",
    "    Input: DataFrame to aggregate; list of lists that describes the procedure to aggregate\n",
    "    \n",
    "    Output: DataFrame of aggregated data\n",
    "    """\n",
    "    @staticmethod\n",
    "    def run_agg_process(df, agg_procedure):\n",
    "        current_step = df.copy()\n",
    "        for step in agg_procedure:\n",
    "            next_step = pd.DataFrame(index=df.index)\n",
    "            \n",
    "            for sub_func in step:\n",
    "                agg_func = getattr(Agg, sub_func)\n",
    "                next_step = pd.concat([next_step, agg_func(current_step)], axis=1)\n",
    "            \n",
    "            current_step = next_step\n",
    "            \n",
    "        return current_step\n",
    "    \n",
    "    """\n",
    "    multi_and_gradient: Aggregate the question-answer pairs based on whether they are multi-select or not level. \n",
    "    Specifically, treat multi-select questions as default and aggregate by each non-multiselect quesion by assigning \n",
    "    value to correseponding answers. For Uncertainty feature, identify answers indicating uncertainty and aggregate \n",
    "    them using a mean into a time series.\n",
    "\n",
    "    Input: DataFrame of 'daily_aggregate.pq'\n",
    "\n",
    "    Output: DataFrame\n",
    "    """\n",
    "    @staticmethod\n",
    "    def multi_and_gradient(df, treat_as_multiselect=TREAT_AS_MULTI, gradient=GRADIENT, uncertainty=UNCERTAINTY):\n",
    "\n",
    "        df_exp = pd.DataFrame(index=df.index)\n",
    "        # 1. Treat \"multiselect\" quesitons as default\n",
    "        if treat_as_multiselect:\n",
    "            temp = []\n",
    "            for qid, aid in treat_as_multiselect.items():\n",
    "                pattern = r'^(' + '|'.join('Q' + str(qid) + 'A' + re.escape(str(id)) for id in aid) + r')'\n",
    "                data = df.filter(regex=pattern)\n",
    "                # Check whether this question is in df\n",
    "                if data.columns.empty:\n",
    "                    continue\n",
    "                else: \n",
    "                    temp.append(data)\n",
    "            df_exp = pd.concat(temp, axis=1)\n",
    "\n",
    "        if gradient:\n",
    "        # 2. Aggregate answers by each non-multiselect quesion\n",
    "            # Select non-multiselect quesion\n",
    "            for qid, aid in gradient.items():\n",
    "                data = df.filter(regex='Q' + str(qid))\n",
    "                \n",
    "                # Check whether this question is in df\n",
    "                if data.columns.empty:\n",
    "                    continue\n",
    "                \n",
    "                # Drop the answers that are regard as \"multi-select\"\n",
    "                if aid:\n",
    "                    lst = ['Q' + str(qid) + 'A' + str(id) for id in aid]\n",
    "                    data = data.drop(columns=lst)\n",
    "\n",
    "                # Calculate weighted average value of each question, by assigning value 1, 2, 3, ... to each answer respectively\n",
    "                length = len(data.columns) + 1\n",
    "                order = np.array(list(range(1, length)))\n",
    "                data = data @ order\n",
    "            \n",
    "                # Standardize\n",
    "                data /= length\n",
    "                \n",
    "                df_exp['Q' + str(qid) + 'Non_Multi'] = data\n",
    "\n",
    "        return df_exp.dropna(axis=1, how='all')\n",
    "    \n",
    "\n",
    "    """\n",
    "    Agg2: Aggregate the answers by subcategory level. \n",
    "    Filter the intra-question aggregated dataframe by each subcategory and perform basic aggregations, including mean and PCA(require non-Nan data)\n",
    "\n",
    "    Input: DataFrame\n",
    "\n",
    "    Output: DataFrame\n",
    "    """\n",
    "    @staticmethod\n",
    "    def agg2(df, subcat=SUBCAT, pca=False, correlation=False):\n",
    "        df_sub_agg = pd.DataFrame(index = df.index)\n",
    "\n",
    "        # Loop through each subcategory\n",
    "        for qid, aid in subcat.items():\n",
    "            # Filter out the data under some specific subcategory\n",
    "            pattern = r'^(' + '|'.join('Q' + re.escape(str(id)) for id in aid) + r')'\n",
    "            data = df.filter(regex = pattern)\n",
    "\n",
    "            if correlation:\n",
    "                print(qid)\n",
    "                display(data.corr())\n",
    "\n",
    "            if pca:\n",
    "                # PCA model require non-NaN data\n",
    "                if data.isna().any().any():\n",
    "                    raise ValueError('There is NaN in the dataframe!')\n",
    "                \n",
    "                # Build the PCA model\n",
    "                if len(data.columns) > 1:\n",
    "                    pca_model = PCA()\n",
    "                    pca_model.fit(data)\n",
    "                    transformed_data = pca_model.transform(data)\n",
    "                    # choose the component with the largest variance as the aggregation outcome of the subcategory\n",
    "                    df_sub_agg[qid] = pd.DataFrame(transformed_data, index=df.index).iloc[:,0]\n",
    "\n",
    "                else:\n",
    "                    df_sub_agg[qid] = data\n",
    "            else:\n",
    "                # Take the mean as the aggregation outcome of the subcategory\n",
    "                df_sub_agg[qid] = data.mean(axis=1)\n",
    "                \n",
    "        return df_sub_agg\n",
    "    \n",
    "    @staticmethod\n",
    "    def uncertainty_agg_groups(df):\n",
    "        return Agg.uncertainty_agg_helper(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def uncertainty_agg_no_groups(df):\n",
    "        return Agg.uncertainty_agg_helper(df, groups=False)\n",
    "    \n",
    "    @staticmethod\n",
    "    def uncertainty_agg_helper(df, uncertainty=UNCERTAINTY, uncertainty_groups=UNCERTAINTY_GROUPS, groups=True):\n",
    "        \n",
    "        # This function does not drop the first row to keep formatting consistent for future functions,\n",
    "        # but doing so would be beneficial since the first row doesn't seem to contain healthy data\n",
    "        \n",
    "        # Note that this function does not smooth, but this aggregation works best with a multi-day rolling average\n",
    "        \n",
    "        # Filter for only the uncertainty questions\n",
    "        pattern = r'^(' + '|'.join('Q' + str(qid) + 'A' + re.escape(str(aid)) for qid, aid in uncertainty.items()) + r')'\n",
    "        uncert = df.filter(regex=pattern)\n",
    "        \n",
    "        # Retrieve the expected percentages of the uncertain response (calculated by taking 1 / # of possible answers)\n",
    "        ans_count = {}\n",
    "        for c in df.columns:\n",
    "            prefix = c.split('A')[0][1:]\n",
    "            if int(prefix) in uncertainty.keys():\n",
    "                ans_count[prefix] = ans_count.get(prefix, 0) + 1\n",
    "        normal_percentage = {key: 1 / value for key, value in ans_count.items()}\n",
    "        \n",
    "        # Convert values to excess values by subtracting expected percentages\n",
    "        for key, value in normal_percentage.items():\n",
    "            cols = [col for col in uncert.columns if col.startswith('Q' + key)]\n",
    "            for col in cols:\n",
    "                uncert[col] -= value\n",
    "        \n",
    "        # Construct output dataframe\n",
    "        output = pd.DataFrame(index=uncert.index)\n",
    "        \n",
    "        if not groups:\n",
    "            output['excess mean'] = uncert.mean(axis=1)\n",
    "            return output\n",
    "        \n",
    "        # This part is for if we want to split by subcategory groups\n",
    "        uncert.columns = uncert.columns.str.extract(r'Q(\\d+)A').squeeze()\n",
    "        \n",
    "        for key in uncertainty_groups:\n",
    "            group = uncertainty_groups[key]\n",
    "            group_df = uncert[group]\n",
    "            output[key + ' excess mean'] = group_df.mean(axis=1)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def subcat_agg(df, subcat=SUBCAT):\n",
    "        output = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        for cat, qs in subcat.items():\n",
    "            regex_pattern = r'^Q(?:' + '|'.join(map(str, qs)) + r')[A-Za-z].*'\n",
    "            cat_filt = df.filter(regex=regex_pattern, axis=1)\n",
    "            df[cat] = cat_filt.mean(axis=1)\n",
    "    \n",
    "        regex_pattern = r'^Q\\d+[A-Za-z].*'\n",
    "        non_q_cols = df.columns[~df.columns.str.contains(regex_pattern)]\n",
    "        non_q_df = df.loc[:, non_q_cols]\n",
    "        \n",
    "        return pd.concat([output, non_q_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Economic Expectations excess mean</th>\n",
       "      <th>Economic Trends excess mean</th>\n",
       "      <th>Inflation excess mean</th>\n",
       "      <th>Economic Expectations</th>\n",
       "      <th>Economic Trends</th>\n",
       "      <th>Employment</th>\n",
       "      <th>Employment Effects</th>\n",
       "      <th>General Shopping Trends</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Home Improvement</th>\n",
       "      <th>Home Ownership</th>\n",
       "      <th>Home Value Prediction</th>\n",
       "      <th>Inflation</th>\n",
       "      <th>Online Retail</th>\n",
       "      <th>Personal Finances</th>\n",
       "      <th>Physical Retail</th>\n",
       "      <th>Spending Expectations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-07-29</th>\n",
       "      <td>0.018189</td>\n",
       "      <td>-0.021127</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.285737</td>\n",
       "      <td>0.490657</td>\n",
       "      <td>0.124285</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.395269</td>\n",
       "      <td>0.417814</td>\n",
       "      <td>0.350962</td>\n",
       "      <td>0.423956</td>\n",
       "      <td>0.374021</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.602358</td>\n",
       "      <td>0.432123</td>\n",
       "      <td>0.568560</td>\n",
       "      <td>0.528050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-30</th>\n",
       "      <td>0.076054</td>\n",
       "      <td>-0.019828</td>\n",
       "      <td>-0.037079</td>\n",
       "      <td>0.298204</td>\n",
       "      <td>0.496820</td>\n",
       "      <td>0.122152</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.385768</td>\n",
       "      <td>0.414934</td>\n",
       "      <td>0.347299</td>\n",
       "      <td>0.423026</td>\n",
       "      <td>0.378684</td>\n",
       "      <td>0.495431</td>\n",
       "      <td>0.616007</td>\n",
       "      <td>0.433680</td>\n",
       "      <td>0.573958</td>\n",
       "      <td>0.530486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-31</th>\n",
       "      <td>0.087482</td>\n",
       "      <td>-0.029110</td>\n",
       "      <td>-0.048522</td>\n",
       "      <td>0.292592</td>\n",
       "      <td>0.492415</td>\n",
       "      <td>0.120745</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.377879</td>\n",
       "      <td>0.409727</td>\n",
       "      <td>0.361328</td>\n",
       "      <td>0.422917</td>\n",
       "      <td>0.364881</td>\n",
       "      <td>0.491270</td>\n",
       "      <td>0.600375</td>\n",
       "      <td>0.430079</td>\n",
       "      <td>0.582072</td>\n",
       "      <td>0.528463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-01</th>\n",
       "      <td>0.080301</td>\n",
       "      <td>-0.032164</td>\n",
       "      <td>-0.043358</td>\n",
       "      <td>0.299616</td>\n",
       "      <td>0.498611</td>\n",
       "      <td>0.127724</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.382094</td>\n",
       "      <td>0.411871</td>\n",
       "      <td>0.362868</td>\n",
       "      <td>0.423222</td>\n",
       "      <td>0.378233</td>\n",
       "      <td>0.495851</td>\n",
       "      <td>0.610394</td>\n",
       "      <td>0.431783</td>\n",
       "      <td>0.580798</td>\n",
       "      <td>0.531406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-02</th>\n",
       "      <td>0.075973</td>\n",
       "      <td>-0.022422</td>\n",
       "      <td>-0.049776</td>\n",
       "      <td>0.297309</td>\n",
       "      <td>0.494170</td>\n",
       "      <td>0.130662</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.374572</td>\n",
       "      <td>0.418726</td>\n",
       "      <td>0.370614</td>\n",
       "      <td>0.424785</td>\n",
       "      <td>0.387178</td>\n",
       "      <td>0.510430</td>\n",
       "      <td>0.626844</td>\n",
       "      <td>0.434551</td>\n",
       "      <td>0.588275</td>\n",
       "      <td>0.527218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-31</th>\n",
       "      <td>0.090519</td>\n",
       "      <td>-0.009740</td>\n",
       "      <td>-0.089610</td>\n",
       "      <td>0.301407</td>\n",
       "      <td>0.492749</td>\n",
       "      <td>0.121731</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.399706</td>\n",
       "      <td>0.402015</td>\n",
       "      <td>0.342153</td>\n",
       "      <td>0.425211</td>\n",
       "      <td>0.357444</td>\n",
       "      <td>0.520130</td>\n",
       "      <td>0.579265</td>\n",
       "      <td>0.440236</td>\n",
       "      <td>0.573434</td>\n",
       "      <td>0.530549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-01</th>\n",
       "      <td>0.108106</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>-0.057827</td>\n",
       "      <td>0.303770</td>\n",
       "      <td>0.488126</td>\n",
       "      <td>0.117429</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.402042</td>\n",
       "      <td>0.411542</td>\n",
       "      <td>0.351307</td>\n",
       "      <td>0.424687</td>\n",
       "      <td>0.356250</td>\n",
       "      <td>0.506709</td>\n",
       "      <td>0.597914</td>\n",
       "      <td>0.446150</td>\n",
       "      <td>0.603268</td>\n",
       "      <td>0.530008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-02</th>\n",
       "      <td>0.104156</td>\n",
       "      <td>0.021324</td>\n",
       "      <td>-0.067003</td>\n",
       "      <td>0.293401</td>\n",
       "      <td>0.481987</td>\n",
       "      <td>0.119979</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.383123</td>\n",
       "      <td>0.416710</td>\n",
       "      <td>0.364000</td>\n",
       "      <td>0.425509</td>\n",
       "      <td>0.342297</td>\n",
       "      <td>0.505649</td>\n",
       "      <td>0.611624</td>\n",
       "      <td>0.427373</td>\n",
       "      <td>0.591169</td>\n",
       "      <td>0.527076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-03</th>\n",
       "      <td>0.060239</td>\n",
       "      <td>-0.019485</td>\n",
       "      <td>-0.071608</td>\n",
       "      <td>0.313292</td>\n",
       "      <td>0.485682</td>\n",
       "      <td>0.118120</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.392299</td>\n",
       "      <td>0.395727</td>\n",
       "      <td>0.351601</td>\n",
       "      <td>0.426772</td>\n",
       "      <td>0.347481</td>\n",
       "      <td>0.514475</td>\n",
       "      <td>0.626529</td>\n",
       "      <td>0.440371</td>\n",
       "      <td>0.594591</td>\n",
       "      <td>0.531878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-04</th>\n",
       "      <td>0.105532</td>\n",
       "      <td>0.023904</td>\n",
       "      <td>-0.055578</td>\n",
       "      <td>0.298845</td>\n",
       "      <td>0.479333</td>\n",
       "      <td>0.120265</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.375166</td>\n",
       "      <td>0.414599</td>\n",
       "      <td>0.353610</td>\n",
       "      <td>0.425527</td>\n",
       "      <td>0.366379</td>\n",
       "      <td>0.513369</td>\n",
       "      <td>0.601986</td>\n",
       "      <td>0.443263</td>\n",
       "      <td>0.602268</td>\n",
       "      <td>0.529753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Economic Expectations excess mean  Economic Trends excess mean  \\\n",
       "2022-07-29                           0.018189                    -0.021127   \n",
       "2022-07-30                           0.076054                    -0.019828   \n",
       "2022-07-31                           0.087482                    -0.029110   \n",
       "2022-08-01                           0.080301                    -0.032164   \n",
       "2022-08-02                           0.075973                    -0.022422   \n",
       "...                                       ...                          ...   \n",
       "2023-08-31                           0.090519                    -0.009740   \n",
       "2023-09-01                           0.108106                     0.012780   \n",
       "2023-09-02                           0.104156                     0.021324   \n",
       "2023-09-03                           0.060239                    -0.019485   \n",
       "2023-09-04                           0.105532                     0.023904   \n",
       "\n",
       "            Inflation excess mean  Economic Expectations  Economic Trends  \\\n",
       "2022-07-29              -0.200000               0.285737         0.490657   \n",
       "2022-07-30              -0.037079               0.298204         0.496820   \n",
       "2022-07-31              -0.048522               0.292592         0.492415   \n",
       "2022-08-01              -0.043358               0.299616         0.498611   \n",
       "2022-08-02              -0.049776               0.297309         0.494170   \n",
       "...                           ...                    ...              ...   \n",
       "2023-08-31              -0.089610               0.301407         0.492749   \n",
       "2023-09-01              -0.057827               0.303770         0.488126   \n",
       "2023-09-02              -0.067003               0.293401         0.481987   \n",
       "2023-09-03              -0.071608               0.313292         0.485682   \n",
       "2023-09-04              -0.055578               0.298845         0.479333   \n",
       "\n",
       "            Employment  Employment Effects  General Shopping Trends   Grocery  \\\n",
       "2022-07-29    0.124285            0.166667                 0.395269  0.417814   \n",
       "2022-07-30    0.122152            0.166667                 0.385768  0.414934   \n",
       "2022-07-31    0.120745            0.166667                 0.377879  0.409727   \n",
       "2022-08-01    0.127724            0.166667                 0.382094  0.411871   \n",
       "2022-08-02    0.130662            0.166667                 0.374572  0.418726   \n",
       "...                ...                 ...                      ...       ...   \n",
       "2023-08-31    0.121731            0.166667                 0.399706  0.402015   \n",
       "2023-09-01    0.117429            0.166667                 0.402042  0.411542   \n",
       "2023-09-02    0.119979            0.166667                 0.383123  0.416710   \n",
       "2023-09-03    0.118120            0.166667                 0.392299  0.395727   \n",
       "2023-09-04    0.120265            0.166667                 0.375166  0.414599   \n",
       "\n",
       "            Home Improvement  Home Ownership  Home Value Prediction  \\\n",
       "2022-07-29          0.350962        0.423956               0.374021   \n",
       "2022-07-30          0.347299        0.423026               0.378684   \n",
       "2022-07-31          0.361328        0.422917               0.364881   \n",
       "2022-08-01          0.362868        0.423222               0.378233   \n",
       "2022-08-02          0.370614        0.424785               0.387178   \n",
       "...                      ...             ...                    ...   \n",
       "2023-08-31          0.342153        0.425211               0.357444   \n",
       "2023-09-01          0.351307        0.424687               0.356250   \n",
       "2023-09-02          0.364000        0.425509               0.342297   \n",
       "2023-09-03          0.351601        0.426772               0.347481   \n",
       "2023-09-04          0.353610        0.425527               0.366379   \n",
       "\n",
       "            Inflation  Online Retail  Personal Finances  Physical Retail  \\\n",
       "2022-07-29   0.483333       0.602358           0.432123         0.568560   \n",
       "2022-07-30   0.495431       0.616007           0.433680         0.573958   \n",
       "2022-07-31   0.491270       0.600375           0.430079         0.582072   \n",
       "2022-08-01   0.495851       0.610394           0.431783         0.580798   \n",
       "2022-08-02   0.510430       0.626844           0.434551         0.588275   \n",
       "...               ...            ...                ...              ...   \n",
       "2023-08-31   0.520130       0.579265           0.440236         0.573434   \n",
       "2023-09-01   0.506709       0.597914           0.446150         0.603268   \n",
       "2023-09-02   0.505649       0.611624           0.427373         0.591169   \n",
       "2023-09-03   0.514475       0.626529           0.440371         0.594591   \n",
       "2023-09-04   0.513369       0.601986           0.443263         0.602268   \n",
       "\n",
       "            Spending Expectations  \n",
       "2022-07-29               0.528050  \n",
       "2022-07-30               0.530486  \n",
       "2022-07-31               0.528463  \n",
       "2022-08-01               0.531406  \n",
       "2022-08-02               0.527218  \n",
       "...                           ...  \n",
       "2023-08-31               0.530549  \n",
       "2023-09-01               0.530008  \n",
       "2023-09-02               0.527076  \n",
       "2023-09-03               0.531878  \n",
       "2023-09-04               0.529753  \n",
       "\n",
       "[403 rows x 17 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate demo\n",
    "\n",
    "df = pd.read_pickle('dadf_relevant.pkl').asfreq('d')\n",
    "Agg.run_agg_process(df, [['multi_and_gradient', 'uncertainty_agg_groups'], ['subcat_agg']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS METHODS TO BE IMPLEMENTED, NO CURRENT FILTERING\n",
    "class Filter():\n",
    "    def __init__(self, combinations):\n",
    "        self.combinations = combinations\n",
    "        \n",
    "    """\n",
    "    Filter1: Put description here.\n",
    "    """\n",
    "    @staticmethod\n",
    "    def filter1(df):\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter2(df):\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter3(df):\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Smooth():\n",
    "    def __init__(self, combinations):\n",
    "        self.combinations = combinations\n",
    "        \n",
    "    """\n",
    "    Smooth1: Moving Average\n",
    "    Takes in a dataframe and returns the moving average based on the window size.\n",
    "\n",
    "    Input: \n",
    "\n",
    "    Output:\n",
    "    """\n",
    "    @staticmethod\n",
    "    def smooth1(df, window=12):\n",
    "        moving_avg = df.rolling(window=window).mean()\n",
    "        return moving_avg\n",
    "    \n",
    "\n",
    "    """\n",
    "    Smooth2: Exponential Smoothing\n",
    "    Takes in a dataframe column and returns an exponentially smoothed column. Can choose to add trend\n",
    "    or seasonality fit based on parameters \"is_seasonal\" and \"is_trend\". For seasonality, can input a \n",
    "    seasonality period (with default being 12). Can choose the start and end point for the prediction values.\n",
    "    \n",
    "    Input: \n",
    "\n",
    "    Output:\n",
    "    """    \n",
    "    ### RUNS INTO ISSUES WITH NAN VALUES -> DOES NOT RETURN FULL DF UNLESS NO NANs\n",
    "    ### ALSO RUNS INTO ISSUE WHEN NO DATAFRAME FREQUENCY\n",
    "    @staticmethod\n",
    "    def smooth2(df, seasonal_periods=12, is_seasonal=None, is_trend=None):\n",
    "        def exponential_smooth(col, seasonal_periods=12, is_seasonal=None, is_trend=None):\n",
    "            if is_seasonal:\n",
    "                is_seasonal = 'add'\n",
    "            if is_trend:\n",
    "                is_trend = 'add'\n",
    "\n",
    "            model = ExponentialSmoothing(col, seasonal_periods=seasonal_periods, trend=is_trend, seasonal=is_seasonal).fit()\n",
    "\n",
    "            return model.predict(start=0, end=len(col)-1)\n",
    "        return df.apply(exponential_smooth)    \n",
    "\n",
    "    """\n",
    "    Smooth3: Lowess Smoothing\n",
    "    Takes in a dataframe column and returns an lowess smoothed column. Fraction parameter set to a\n",
    "    default value of 0.1.\n",
    "\n",
    "    Input: \n",
    "\n",
    "    Output:\n",
    "    """\n",
    "    ### RUNS INTO ISSUES WITH NAN VALUES -> WILL PRODUCE ERROR IN RUN_MODEL METHOD\n",
    "    @staticmethod\n",
    "    def smooth3(df):\n",
    "        def sm_lowess(col, frac=0.1):\n",
    "            # smoothed = lowess(col, range(len(col)), frac)\n",
    "            smoothed = pd.Series(lowess(col, range(len(col)), 0.01)[:, 1], index=col.dropna().index).reindex(col.index)\n",
    "            # print(smoothed)\n",
    "            return smoothed\n",
    "\n",
    "        return df.apply(sm_lowess)\n",
    "    \n",
    "    """\n",
    "    Smooth4: Fourier Smoothing\n",
    "    Takes in a dataframe column and returns an fourier smoothed column. Fraction parameter set to a\n",
    "    default value of 0.02.\n",
    "\n",
    "    Input: \n",
    "\n",
    "    Output:\n",
    "    """\n",
    "    def fourier_smooth(data: pd.DataFrame, cutoff_freq=0.02):\n",
    "\n",
    "        new_data = pd.DataFrame()\n",
    "        for col in data.columns:\n",
    "            fourier_transform = fft(data[col].dropna().values)\n",
    "            \n",
    "            n = len(data[col])\n",
    "            freqs = np.fft.fftfreq(n)\n",
    "            cutoff_index = int(cutoff_freq * n)\n",
    "            \n",
    "            fourier_transform[cutoff_index:-cutoff_index] = 0\n",
    "            \n",
    "            smoothed_data = np.real(ifft(fourier_transform))\n",
    "            smoothed_data = pd.Series(smoothed_data, index=data[col].dropna().index)\n",
    "            new_data[col] = smoothed_data\n",
    "        new_data.index = data.index\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSAggregation():\n",
    "    \n",
    "    def __init__(self, combinations):\n",
    "        self.combinations=combinations\n",
    "\n",
    "    @staticmethod\n",
    "    def myagg1(df):\n",
    "        return np.mean(df, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def myagg2(df):\n",
    "        return np.median(df, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def myagg3(df):\n",
    "        df=df.fillna(0)\n",
    "        scaler = StandardScaler()\n",
    "        data_standardized = scaler.fit_transform(df)\n",
    "\n",
    "        # Perform hierarchical clustering\n",
    "        Z = linkage(data_standardized, method='ward')\n",
    "\n",
    "        # Determine the number of clusters via dendrogram or other method\n",
    "        # Here, we still use 3 for illustration\n",
    "        num_clusters = 3\n",
    "        clusters = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "\n",
    "        # Aggregate data within each cluster and compute cluster sizes\n",
    "        cluster_aggregates = []\n",
    "        cluster_sizes = []\n",
    "        for k in range(1, num_clusters+1):\n",
    "            cluster_data = data_standardized[clusters == k]\n",
    "            cluster_aggregates.append(cluster_data.mean(axis=0))\n",
    "            cluster_sizes.append(len(cluster_data))\n",
    "\n",
    "        # Convert lists to arrays for vectorized operations\n",
    "        cluster_aggregates = np.array(cluster_aggregates)\n",
    "        cluster_sizes = np.array(cluster_sizes)\n",
    "\n",
    "        # Compute weights for each cluster based on its size\n",
    "        cluster_weights = cluster_sizes / cluster_sizes.sum()\n",
    "\n",
    "        # Perform weighted aggregation of cluster centroids\n",
    "        weighted_aggregate = np.dot(cluster_weights, cluster_aggregates)\n",
    "\n",
    "        # Reverse the standardization to bring the aggregated row back to the original data scale\n",
    "        final_aggregated_row_original = scaler.inverse_transform(weighted_aggregate.reshape(1,-1))\n",
    "\n",
    "        return final_aggregated_row_original\n",
    "\n",
    "    @staticmethod\n",
    "    def myagg4(df):\n",
    "        df=df.fillna(0)\n",
    "        scaler = StandardScaler()\n",
    "        data_standardized = scaler.fit_transform(df)\n",
    "    \n",
    "        kmeans = KMeans(n_clusters=3, random_state=42).fit(data_standardized)\n",
    "        labels = kmeans.labels_\n",
    "\n",
    "        # Calculate the size (number of points) in each cluster\n",
    "        cluster_sizes = np.bincount(labels)\n",
    "\n",
    "        # Normalize the sizes to get weights\n",
    "        # We add a small constant to avoid division by zero in case of very small clusters\n",
    "        cluster_weights = cluster_sizes / (cluster_sizes.sum() + 1e-10)\n",
    "\n",
    "        # Step 2: Compute the centroid of each cluster\n",
    "        cluster_centroids = np.array([data_standardized[labels == i].mean(axis=0) for i in range(3)])\n",
    "\n",
    "        # Step 3: Compute weighted centroids\n",
    "        # Multiply each centroid by its cluster's weight\n",
    "        weighted_centroids = cluster_centroids * cluster_weights[:, np.newaxis]\n",
    "\n",
    "        # Step 4: Aggregate these weighted centroids to a single row\n",
    "        aggregated_row = weighted_centroids.sum(axis=0)\n",
    "\n",
    "        # Reverse the standardization to bring the aggregated row back to the original data scale\n",
    "        aggregated_row_original = scaler.inverse_transform(aggregated_row.reshape(1,-1))\n",
    "\n",
    "        return aggregated_row_original\n",
    "\n",
    "    @staticmethod\n",
    "    def myagg5(df):\n",
    "        alpha = 0.1\n",
    "\n",
    "        weights = np.array([alpha * (1 - alpha) ** i for i in range(30)][::-1])\n",
    "\n",
    "        weights /= weights.sum()\n",
    "    \n",
    "        weighted_data = df * weights[:, None]  \n",
    "\n",
    "        aggregated_row = weighted_data.sum(axis=0)\n",
    "    \n",
    "        return aggregated_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test():\n",
    "    linear_reg = LinearRegression()\n",
    "    stepwise_reg = StepwiseRegression(criterion='bic', verbose=False)\n",
    "    lasso = Lasso(alpha=0.01)\n",
    "    xgboosting = xgb.XGBRegressor(objective ='reg:squarederror', \n",
    "                         colsample_bytree = 0.3, \n",
    "                         learning_rate = 0.1,\n",
    "                         max_depth = 5, \n",
    "                         alpha = 10, \n",
    "                         n_estimators = 10)\n",
    "    def __init__(self, combinations):\n",
    "        self.combinations = combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    """\n",
    "    Run through a bunch of different processes based on combinations of agg, filt, etc. agg, filt, and smooth are\n",
    "    instances of the Agg, Filter, and Smooth classes. Each class is initialized with a set of combinations of which\n",
    "    functions we want to test together. The combination attribute will be used to test different combinations of these\n",
    "    operatios.\n",
    "    """\n",
    "    \n",
    "    \n",
    "    def __init__(self, agg, filt, smooth, tsagg, test, lookback_period=12, base_path='../data'):\n",
    "        \n",
    "        self.agg = agg\n",
    "        self.filt = filt\n",
    "        self.smooth = smooth\n",
    "        self.test = test\n",
    "        self.tsagg = tsagg\n",
    "        self.lookback_period = lookback_period\n",
    "        self.base_path = base_path\n",
    "        \n",
    "\n",
    "    def read_and_format_data(self):\n",
    "        \n",
    "        """\n",
    "        Read in time series of question responses from the daily_aggregate.parquet and\n",
    "        initial_indicator_dataset.csv files.\n",
    "\n",
    "        Args:\n",
    "            base_path (str): The base path for data files.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame, pd.DataFrame: A tuple of DataFrames containing daily aggregate data and indicator data.\n",
    "        """\n",
    "        # Read daily aggregate file (predictor library)\n",
    "        df_agg = pd.read_parquet(join(self.base_path,'daily_aggregate.pq'))\n",
    "\n",
    "        # Read indicator data file (target library)\n",
    "        df_ind = pd.read_csv(join(self.base_path,'initial_indicator_dataset.csv'))\n",
    "\n",
    "        # Process indicator data\n",
    "        df_ind['Date'] = pd.to_datetime(df_ind['Date']) # convert dates to datetimes\n",
    "        df_ind = df_ind.set_index('Date') # set the date column as the index\n",
    "        \n",
    "        self.df_agg = df_agg\n",
    "        self.df_ind = df_ind\n",
    "        \n",
    "        return\n",
    "    \n",
    "\n",
    "        \n",
    "    def data_filter(self, **kwargs):\n",
    "        """\n",
    "            Filter data using a rolling window.\n",
    "\n",
    "            Args:\n",
    "                df (pd.DataFrame): The input DataFrame to be filtered.\n",
    "                kwargs: will be filter specific arguments; we can expand the filter\n",
    "                list/method later\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: The filtered DataFrame.\n",
    "        """\n",
    "        window_size = kwargs['window_size']     # set window size for moving average filter\n",
    "        df_smooth = self.df_agg.rolling(window_size).mean()   # apply moving average filter\n",
    "        df_smooth = df_smooth.tail(-window_size+1)\n",
    "        \n",
    "        return df_smooth\n",
    "\n",
    "    \n",
    "    def get_combined_data_with_given_indicator(self, df_indicator_all, df_daily_agg, tsagg_func, indicator_col ='CONSSENT Index'):\n",
    "        """\n",
    "            Combine indicator data with daily aggregate data. Also as indicator data is monthly change the\n",
    "            daily aggregate data to monthly.\n",
    "\n",
    "            Args:\n",
    "                df_indicator_all (pd.DataFrame): The indicator data.\n",
    "                df_daily_agg (pd.DataFrame): The daily aggregate data.\n",
    "                indicator_col (str): The name of the indicator column to consider.\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: Combined DataFrame with monthly aggregated data.\n",
    "        """\n",
    "        df_indicator = df_indicator_all[indicator_col]  # restrict the indicators df on a single indicator\n",
    "        df_indicator = df_indicator[df_indicator.diff() != 0].tail(-1)  # get the dates when new values of the\n",
    "        # indicator arrived (e.g. release dates)\n",
    "        # and drop the initial date since we do\n",
    "        # not know precisely when it was released\n",
    "\n",
    "        # Define the start and end_dates of the aggregation periods\n",
    "        start_dates = df_indicator.index[:-1]  # list of start dates\n",
    "        end_dates = (df_indicator.index - datetime.timedelta(days=1))[1:]  # note that we do not\n",
    "        # include release date here\n",
    "        range_pairs = zip(start_dates, end_dates)\n",
    "\n",
    "        ## Slow Loop Aggregator (see if we can find a way to update with pd.groupby()\n",
    "        # explore ideas like: df_smooth.groupby(pd.cut(start_dates,end_dates)).agg(myagg)\n",
    "\n",
    "        monthly_agg_values = []\n",
    "        for stday, endday in range_pairs:\n",
    "            df_month = df_daily_agg[stday:endday]\n",
    "            # TODO explore variations here, e.g. last value, exponential moving average, etc.\n",
    "            agg_month_val = TSAggregation.tsagg_func(df_month)\n",
    "            monthly_agg_values.append(agg_month_val)\n",
    "        df_monthly_agg = pd.concat(monthly_agg_values, axis=1)\n",
    "        df_monthly_agg.columns = end_dates\n",
    "        df_monthly_agg = df_monthly_agg.T\n",
    "        df_monthly_agg = df_monthly_agg.dropna(axis=0, how='all')  # drop rows with all nans\n",
    "        df_monthly_agg.index = df_monthly_agg.index + datetime.timedelta(days=1)\n",
    "\n",
    "        # combined aligned df\n",
    "        df_monthly_combined = pd.concat([df_monthly_agg, df_indicator], axis=1)\n",
    "        df_monthly_combined = df_monthly_combined[df_monthly_agg.index.min():]  # filter prior to the first question date\n",
    "        return df_monthly_combined\n",
    "\n",
    "    \n",
    "    \n",
    "    def run(self):\n",
    "        pipeline_dict = {}\n",
    "        combo_list = [self.agg.combinations, self.filt.combinations, self.smooth.combinations, self.tsagg.combinations, self.test.combinations]\n",
    "        for combination in itertools.product(*combo_list):\n",
    "            if hasattr(Agg, combination[0]):\n",
    "                agg_func = getattr(Agg, combination[0])\n",
    "            else:\n",
    "                raise InvalidOperation\n",
    "            \n",
    "            if hasattr(Filter, combination[1]):\n",
    "                filt_func = getattr(Filter, combination[1])\n",
    "            else:\n",
    "                raise InvalidOperation\n",
    "                \n",
    "            if hasattr(Smooth, combination[2]):\n",
    "                smooth_func = getattr(Smooth, combination[2])\n",
    "            else:\n",
    "                raise InvalidOperation\n",
    "                \n",
    "            if hasattr(Test, combination[3]):\n",
    "                tsagg_func = getattr(TSAgg, combination[3])\n",
    "            else:\n",
    "                raise InvalidOperation\n",
    "\n",
    "            if hasattr(Test, combination[4]):\n",
    "                tester = getattr(Test, combination[4])\n",
    "            else:\n",
    "                raise InvalidOperation\n",
    "            \n",
    "            df_ind = self.df_ind.copy()\n",
    "            df_agg = self.df_agg.copy()\n",
    "\n",
    "            reg_final_stats_df = pd.DataFrame(index=df_ind.columns, columns=['rsquared', 'mae', 'mape', 'p_value'])\n",
    "            reg_final_stats_df_pct_change = pd.DataFrame(index=df_ind.columns, columns=['rsquared', 'mae', 'mape', 'p_value'])\n",
    "\n",
    "            for indicator in df_ind.columns[:5]:\n",
    "                # Get the combined monthly survey data with indicator data\n",
    "                \n",
    "                print(f\"Indicator: {indicator}\")\n",
    "                if pd.api.types.is_numeric_dtype(df_ind[indicator]):\n",
    "\n",
    "                    df_comb = self.get_combined_data_with_given_indicator(df_ind, df_agg, tsagg_func, indicator_col=indicator)\n",
    "                    # df_comb_processed = df_comb[df_comb.iloc[:,:-1] <= 1].replace(np.inf, 0)\n",
    "\n",
    "                    df_comb_processed_change = df_comb.pct_change().replace(np.inf, 0)\n",
    "                    \n",
    "                    print(f'Running {tester} model')\n",
    "\n",
    "                    combo = Combo(df = df_comb_processed_change, agg_func = agg_func, filt_func = filt_func,\n",
    "                                  smooth_func = smooth_func, tsagg_func = tsagg_func, tester=tester, lookback_period = self.lookback_period)\n",
    "                    \n",
    "                    combo.process_df()\n",
    "\n",
    "                    reg_final_stats_df_pct_change.loc[indicator], pct_change_reg_coefs_df = combo.run_model(\n",
    "                                                                                            model=tester)\n",
    "                    pipeline_dict[indicator] = reg_final_stats_df_pct_change.loc[indicator]\n",
    "                    df_comb_processed_change.to_csv(f'../results/{indicator}_combined_monthly_processed_data.csv')\n",
    "                    pct_change_reg_coefs_df.to_csv(f'../results/{indicator}_reg_pct_change_coefs.csv')\n",
    "                    \n",
    "            reg_final_stats_df_pct_change.to_csv('../results/reg_pct_change_final_stats.csv')\n",
    "\n",
    "\n",
    "        return pipeline_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_metric(pipeline_dict, metric_name, combination_criteria=()):\n",
    "    \"\"\"\n",
    "    Find the best metric (R^2, MAE, or MAPE) from the pipeline_dict based on a specific combination criteria.\n",
    "\n",
    "    :param pipeline_dict: Dictionary of combinations and their metrics.\n",
    "    :param metric_name: The name of the metric to optimize ('r2', 'mae', or 'mape').\n",
    "    :param combination_criteria: A tuple of values to filter combinations (e.g., ('agg1', 'smooth1')).\n",
    "    :return: A tuple of the best combination and its metrics.\n",
    "    \"\"\"\n",
    "    # Mapping the metric name to its index in the metrics tuple.\n",
    "    metric_index = {'r2': 0, 'mae': 1, 'mape': 2}\n",
    "\n",
    "    if metric_name not in metric_index:\n",
    "        raise ValueError(\"Invalid metric name. Choose from 'r2', 'mae', or 'mape'.\")\n",
    "\n",
    "    best_combination = None\n",
    "    best_metric = float('-inf') if metric_name == 'r2' else float('inf')\n",
    "\n",
    "    for combination, metrics in pipeline_dict.items():\n",
    "        # Check if all elements in combination_criteria are in the current combination.\n",
    "        if not all(elem in combination for elem in combination_criteria):\n",
    "            continue\n",
    "\n",
    "        metric_value = metrics[metric_index[metric_name]]\n",
    "        # Update best metric and combination based on the metric type.\n",
    "        if (metric_name == 'r2' and metric_value > best_metric) or \\\n",
    "           (metric_name in ['mae', 'mape'] and metric_value < best_metric):\n",
    "            best_metric = metric_value\n",
    "            best_combination = combination\n",
    "\n",
    "    return print(f'The best combination with your criteria is {best_combination} with metric {best_metric}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_methods(cls):\n",
    "    """\n",
    "    Function to get a list of class methods. \n",
    "    """\n",
    "    methods = [name for name, method in inspect.getmembers(cls, predicate=inspect.isfunction) if name != '__init__']\n",
    "    return methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indicator: CONSSENT Index\n",
      "Running Lasso(alpha=0.01) model\n",
      "R-squared: -3.7433593194074453, MAE: 0.0013574363047924394, MAPE: 194991026209.4195, p_value: 0.9999999999999999\n",
      "Indicator: CONCCONF Index\n",
      "Running Lasso(alpha=0.01) model\n",
      "R-squared: -3.8541200992334987, MAE: 0.0012853339592843146, MAPE: 187219644730.10126, p_value: 0.9999999999999999\n",
      "Indicator: BASPCCUS Index\n",
      "Running Lasso(alpha=0.01) model\n",
      "R-squared: -8.674792282139938, MAE: 9.004387718948205e-05, MAPE: 174362793005.71765, p_value: 0.9999999999999999\n",
      "Indicator: DELQUS30 Index\n",
      "Running Lasso(alpha=0.01) model\n",
      "R-squared: -2.5390824913842724, MAE: 0.00038539838113972667, MAPE: 75105555153.16043, p_value: 0.9999999999999999\n",
      "Indicator: DELQUS90 Index\n",
      "Running Lasso(alpha=0.01) model\n",
      "R-squared: -6.193806508393243, MAE: 0.0001506742503256926, MAPE: 25767804945.89558, p_value: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ex_agg = Agg(['agg1'])\n",
    "ex_filt = Filter(['filter1'])\n",
    "ex_smooth = Smooth(['smooth1'])\n",
    "ex_tsagg = TSAggregation(['myagg1'])\n",
    "ex_test = Test(['lasso']) # 'linear_reg','lasso', 'stepwise_reg', 'xgboosting'\n",
    "ex_pipeline = Pipeline(ex_agg, ex_filt, ex_smooth, ex_tsagg, ex_test)\n",
    "\n",
    "ex_pipeline.read_and_format_data()\n",
    "# Smoothing the aggregate survey data\n",
    "rolling_window_size = 20\n",
    "df_agg = ex_pipeline.data_filter(window_size=rolling_window_size)\n",
    "\n",
    "\n",
    "x=ex_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best combination with your criteria is None with metric -inf.\n"
     ]
    }
   ],
   "source": [
    "find_best_metric(x, 'r2', ('agg1', 'filter1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Example Pipeline\\nDATA_BASE_PATH = '/Data'\\ndf = pd.read_parquet(join(Constants.DATA_BASE_PATH,'daily_aggregate.pq')) \\n# df = pd.read_parquet('daily_aggregate.pq')\\n\\nex_agg = Agg(['agg1'])\\nex_filt = Filter(['filter1'])\\nex_smooth = Smooth(get_class_methods(Smooth))\\nex_test = Test(['linear_reg']) # 'linear_reg','lasso', 'stepwise_reg', 'xgboosting'\\nex_pipeline = Pipeline(df, ex_agg, ex_filt, ex_smooth, ex_test)\\nex_pipeline.run()\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    """"\n",
    "# Example Pipeline\n",
    "DATA_BASE_PATH = '/Data'\n",
    "df = pd.read_parquet(join(Constants.DATA_BASE_PATH,'daily_aggregate.pq')) \n",
    "# df = pd.read_parquet('daily_aggregate.pq')\n",
    "\n",
    "ex_agg = Agg(['agg1'])\n",
    "ex_filt = Filter(['filter1'])\n",
    "ex_smooth = Smooth(get_class_methods(Smooth))\n",
    "ex_test = Test(['linear_reg']) # 'linear_reg','lasso', 'stepwise_reg', 'xgboosting'\n",
    "ex_pipeline = Pipeline(df, ex_agg, ex_filt, ex_smooth, ex_test)\n",
    "ex_pipeline.run()\n",
    """""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
