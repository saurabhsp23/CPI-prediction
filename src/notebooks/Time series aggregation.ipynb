{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38883f29-249b-4171-a1fa-afd4380eb291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myagg1(x):\n",
    "    """\n",
    "    Example aggregator function that just takes the mean.\n",
    "\n",
    "    Args:\n",
    "        x (pd.DataFrame): Data to be aggregated.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Aggregated data.\n",
    "    """\n",
    "    return np.mean(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234d72ad-5fca-4b70-9578-f66dba85457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myagg2(x):\n",
    "    """\n",
    "    Example aggregator function that just takes the mean.\n",
    "\n",
    "    Args:\n",
    "        x (pd.DataFrame): Data to be aggregated.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Aggregated data.\n",
    "    """\n",
    "    return np.median(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f039e9f4-76ac-4de4-876d-dcae2d8425a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myagg3(x):\n",
    "\n",
    "    x=x.fillna(0)\n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(x)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "    Z = linkage(data_standardized, method='ward')\n",
    "\n",
    "# Determine the number of clusters via dendrogram or other method\n",
    "# Here, we still use 3 for illustration\n",
    "    num_clusters = 3\n",
    "    clusters = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Aggregate data within each cluster and compute cluster sizes\n",
    "    cluster_aggregates = []\n",
    "    cluster_sizes = []\n",
    "    for k in range(1, num_clusters+1):\n",
    "        cluster_data = data_standardized[clusters == k]\n",
    "        cluster_aggregates.append(cluster_data.mean(axis=0))\n",
    "        cluster_sizes.append(len(cluster_data))\n",
    "\n",
    "# Convert lists to arrays for vectorized operations\n",
    "    cluster_aggregates = np.array(cluster_aggregates)\n",
    "    cluster_sizes = np.array(cluster_sizes)\n",
    "\n",
    "# Compute weights for each cluster based on its size\n",
    "    cluster_weights = cluster_sizes / cluster_sizes.sum()\n",
    "\n",
    "# Perform weighted aggregation of cluster centroids\n",
    "    weighted_aggregate = np.dot(cluster_weights, cluster_aggregates)\n",
    "\n",
    "# Reverse the standardization to bring the aggregated row back to the original data scale\n",
    "    final_aggregated_row_original = scaler.inverse_transform(weighted_aggregate.reshape(1,-1))\n",
    "\n",
    "    return final_aggregated_row_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f4b94b-7856-41c6-aa23-d4988bf29f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myagg4(x):\n",
    "    \n",
    "    alpha = 0.1\n",
    "\n",
    "    weights = np.array([alpha * (1 - alpha) ** i for i in range(30)][::-1])\n",
    "\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    weighted_data = x * weights[:, None]  \n",
    "\n",
    "    aggregated_row = weighted_data.sum(axis=0)\n",
    "    \n",
    "    return aggregated_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ee609f-8cdc-4b05-b4fa-044c2554c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myagg5(x):\n",
    "\n",
    "    x=x.fillna(0)\n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(x)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=3, random_state=42).fit(data_standardized)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Calculate the size (number of points) in each cluster\n",
    "    cluster_sizes = np.bincount(labels)\n",
    "\n",
    "    # Normalize the sizes to get weights\n",
    "    # We add a small constant to avoid division by zero in case of very small clusters\n",
    "    cluster_weights = cluster_sizes / (cluster_sizes.sum() + 1e-10)\n",
    "\n",
    "    # Step 2: Compute the centroid of each cluster\n",
    "    cluster_centroids = np.array([data_standardized[labels == i].mean(axis=0) for i in range(3)])\n",
    "\n",
    "    # Step 3: Compute weighted centroids\n",
    "    # Multiply each centroid by its cluster's weight\n",
    "    weighted_centroids = cluster_centroids * cluster_weights[:, np.newaxis]\n",
    "\n",
    "    # Step 4: Aggregate these weighted centroids to a single row\n",
    "    aggregated_row = weighted_centroids.sum(axis=0)\n",
    "\n",
    "    # Reverse the standardization to bring the aggregated row back to the original data scale\n",
    "    aggregated_row_original = scaler.inverse_transform(aggregated_row.reshape(1,-1))\n",
    "\n",
    "    return aggregated_row_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74119723-a097-4d0e-93ab-65b99dfa688a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
